{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def set_global_seed(seed: int = 42):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Simple regression tree (NumPy-only)\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class _TreeNode:\n",
        "    feature_index: Optional[int] = None\n",
        "    threshold: Optional[float] = None\n",
        "    left: Optional['_TreeNode'] = None\n",
        "    right: Optional['_TreeNode'] = None\n",
        "    value: Optional[float] = None\n",
        "\n",
        "class NumpyDecisionTreeRegressor:\n",
        "    def __init__(self, max_depth: int = 2, min_samples_split: int = 2,\n",
        "                 n_thresholds_per_feature: int = 8, random_state: int = 42):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.n_thresholds_per_feature = n_thresholds_per_feature\n",
        "        self.root: Optional[_TreeNode] = None\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        np.random.seed(self.random_state)\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        return np.array([self._predict_row(self.root, row) for row in X])\n",
        "\n",
        "    def _predict_row(self, node: _TreeNode, x_row: np.ndarray) -> float:\n",
        "        while node.value is None:\n",
        "            node = node.left if x_row[node.feature_index] <= node.threshold else node.right\n",
        "        return node.value\n",
        "\n",
        "    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int) -> _TreeNode:\n",
        "        if depth >= self.max_depth or X.shape[0] < self.min_samples_split:\n",
        "            return _TreeNode(value=float(y.mean()))\n",
        "        if np.allclose(y, y[0]):\n",
        "            return _TreeNode(value=float(y[0]))\n",
        "        feat, thr, gain, left_mask, right_mask = self._best_split(X, y)\n",
        "        if feat is None:\n",
        "            return _TreeNode(value=float(y.mean()))\n",
        "        left_node = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        right_node = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "        return _TreeNode(feature_index=feat, threshold=thr, left=left_node, right=right_node)\n",
        "\n",
        "    def _best_split(self, X: np.ndarray, y: np.ndarray):\n",
        "        current_sse = np.sum((y - y.mean()) ** 2)\n",
        "        best_feature, best_threshold, best_gain = None, None, 0.0\n",
        "        best_left_mask, best_right_mask = None, None\n",
        "        for j in range(X.shape[1]):\n",
        "            xj = X[:, j]\n",
        "            if np.unique(xj).size < 2:\n",
        "                continue\n",
        "            thresholds = np.quantile(xj, np.linspace(0, 1, self.n_thresholds_per_feature + 2)[1:-1])\n",
        "            for thr in thresholds:\n",
        "                left_mask = xj <= thr\n",
        "                right_mask = ~left_mask\n",
        "                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:\n",
        "                    continue\n",
        "                yl, yr = y[left_mask], y[right_mask]\n",
        "                sse_left = np.sum((yl - yl.mean()) ** 2)\n",
        "                sse_right = np.sum((yr - yr.mean()) ** 2)\n",
        "                gain = current_sse - (sse_left + sse_right)\n",
        "                if gain > best_gain + 1e-12:\n",
        "                    best_gain, best_feature, best_threshold = gain, j, float(thr)\n",
        "                    best_left_mask, best_right_mask = left_mask, right_mask\n",
        "        if best_feature is None:\n",
        "            return None, None, 0.0, None, None\n",
        "        return best_feature, best_threshold, best_gain, best_left_mask, best_right_mask\n",
        "\n",
        "# -----------------------------\n",
        "# Gradient Boosting Regressor (NumPy-only)\n",
        "# -----------------------------\n",
        "class NumpyGradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators: int = 50, learning_rate: float = 0.1,\n",
        "                 max_depth: int = 2, random_state: int = 42):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.random_state = random_state\n",
        "        self.init_ = 0.0\n",
        "        self.trees: List[NumpyDecisionTreeRegressor] = []\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        np.random.seed(self.random_state)\n",
        "        self.init_ = float(np.mean(y))\n",
        "        y_pred = np.full(X.shape[0], self.init_, dtype=float)\n",
        "        self.trees = []\n",
        "        for m in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            tree = NumpyDecisionTreeRegressor(\n",
        "                max_depth=self.max_depth, min_samples_split=2, n_thresholds_per_feature=8,\n",
        "                random_state=self.random_state + m\n",
        "            )\n",
        "            tree.fit(X, residuals)\n",
        "            y_pred = y_pred + self.learning_rate * tree.predict(X)\n",
        "            self.trees.append(tree)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        y_pred = np.full(X.shape[0], self.init_, dtype=float)\n",
        "        for tree in self.trees:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# -----------------------------\n",
        "# Data: generate -> save -> load -> preprocess\n",
        "# -----------------------------\n",
        "def generate_and_save_dataset(csv_dir=\"data\", csv_name=\"regression_dataset.csv\",\n",
        "                              n_samples=500, n_features=10, n_informative=8, noise=10.0, random_state=42):\n",
        "    ensure_dir(csv_dir)\n",
        "    X, y = make_regression(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        n_informative=n_informative,\n",
        "        noise=noise,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    data = np.concatenate([X, y.reshape(-1, 1)], axis=1)\n",
        "    header = [f\"x{i}\" for i in range(X.shape[1])] + [\"target\"]\n",
        "    path = os.path.join(csv_dir, csv_name)\n",
        "    np.savetxt(path, data, delimiter=\",\", header=\",\".join(header), comments=\"\", fmt=\"%.6f\")\n",
        "    return path\n",
        "\n",
        "def load_and_preprocess_dataset(path: str):\n",
        "    raw = np.genfromtxt(path, delimiter=\",\", names=True)\n",
        "    feature_names = [n for n in raw.dtype.names if n != \"target\"]\n",
        "    X = np.vstack([raw[n] for n in feature_names]).T\n",
        "    y = raw[\"target\"]\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, y\n",
        "\n",
        "# -----------------------------\n",
        "# CV + hyperparameter sweep\n",
        "# -----------------------------\n",
        "def evaluate_models_cv(X: np.ndarray, y: np.ndarray,\n",
        "                       learning_rates=(0.1, 0.05),\n",
        "                       n_estimators_list=(50, 100),\n",
        "                       max_depth_list=(2,),\n",
        "                       n_splits=3, random_state=42):\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    results_custom, results_sklearn = [], []\n",
        "    for lr in learning_rates:\n",
        "        for n_est in n_estimators_list:\n",
        "            for md in max_depth_list:\n",
        "                mae_c, rmse_c, mae_s, rmse_s = [], [], [], []\n",
        "                for tr_idx, va_idx in kf.split(X):\n",
        "                    X_tr, X_va = X[tr_idx], X[va_idx]\n",
        "                    y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "                    gbm = NumpyGradientBoostingRegressor(n_estimators=n_est, learning_rate=lr, max_depth=md)\n",
        "                    gbm.fit(X_tr, y_tr)\n",
        "                    y_pred_c = gbm.predict(X_va)\n",
        "                    mae_c.append(mean_absolute_error(y_va, y_pred_c))\n",
        "                    rmse_c.append(np.sqrt(mean_squared_error(y_va, y_pred_c)))\n",
        "                    sk = GradientBoostingRegressor(loss='squared_error', learning_rate=lr,\n",
        "                                                   n_estimators=n_est, max_depth=md, random_state=random_state)\n",
        "                    sk.fit(X_tr, y_tr)\n",
        "                    y_pred_s = sk.predict(X_va)\n",
        "                    mae_s.append(mean_absolute_error(y_va, y_pred_s))\n",
        "                    rmse_s.append(np.sqrt(mean_squared_error(y_va, y_pred_s)))\n",
        "                results_custom.append({\n",
        "                    'lr': lr, 'n_est': n_est, 'depth': md,\n",
        "                    'mae': np.mean(mae_c), 'rmse': np.mean(rmse_c)\n",
        "                })\n",
        "                results_sklearn.append({\n",
        "                    'lr': lr, 'n_est': n_est, 'depth': md,\n",
        "                    'mae': np.mean(mae_s), 'rmse': np.mean(rmse_s)\n",
        "                })\n",
        "    return results_custom, results_sklearn\n",
        "\n",
        "# -----------------------------\n",
        "# Main execution block\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    set_global_seed(42)\n",
        "\n",
        "    # 1. Generate and save dataset\n",
        "    print(\"Generating and saving dataset...\")\n",
        "    csv_path = generate_and_save_dataset(random_state=42)\n",
        "    print(f\"Dataset saved to {csv_path}\")\n",
        "\n",
        "    # 2. Load and preprocess dataset\n",
        "    print(\"Loading and preprocessing dataset...\")\n",
        "    X, y = load_and_preprocess_dataset(csv_path)\n",
        "    print(f\"Dataset loaded with X shape: {X.shape}, y shape: {y.shape}\")\n",
        "\n",
        "    # 3. Define hyperparameters for sweep\n",
        "    learning_rates = (0.1, 0.05, 0.01)\n",
        "    n_estimators_list = (50, 100, 200)\n",
        "    max_depth_list = (2, 4)\n",
        "\n",
        "    # 4. Run cross-validation and hyperparameter sweep\n",
        "    print(\"Running hyperparameter sweep with 5-fold CV...\")\n",
        "    custom_results, sklearn_results = evaluate_models_cv(\n",
        "        X, y,\n",
        "        learning_rates=learning_rates,\n",
        "        n_estimators_list=n_estimators_list,\n",
        "        max_depth_list=max_depth_list,\n",
        "        n_splits=5, random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Custom GBM Results ---\")\n",
        "    for r in custom_results:\n",
        "        print(f\"LR: {r['lr']}, Estimators: {r['n_est']}, Depth: {r['depth']} => MAE: {r['mae']:.2f}, RMSE: {r['rmse']:.2f}\")\n",
        "\n",
        "    print(\"\\n--- Scikit-learn GBM Results ---\")\n",
        "    for r in sklearn_results:\n",
        "        print(f\"LR: {r['lr']}, Estimators: {r['n_est']}, Depth: {r['depth']} => MAE: {r['mae']:.2f}, RMSE: {r['rmse']:.2f}\")\n",
        "\n",
        "    # 5. Find best models\n",
        "    best_custom = min(custom_results, key=lambda x: x['mae'])\n",
        "    best_sklearn = min(sklearn_results, key=lambda x: x['mae'])\n",
        "\n",
        "    print(\"\\n--- Best Models ---\")\n",
        "    print(f\"Custom GBM Best: LR: {best_custom['lr']}, Estimators: {best_custom['n_est']}, Depth: {best_custom['depth']} | MAE: {best_custom['mae']:.2f}, RMSE: {best_custom['rmse']:.2f}\")\n",
        "    print(f\"Scikit-learn GBM Best: LR: {best_sklearn['lr']}, Estimators: {best_sklearn['n_est']}, Depth: {best_sklearn['depth']} | MAE: {best_sklearn['mae']:.2f}, RMSE: {best_sklearn['rmse']:.2f}\")\n",
        "\n",
        "    # 6. Generate Markdown report\n",
        "    report_content = \"# Gradient Boosting Regressor Comparison Report\\n\\n\"\n",
        "    report_content += \"This report compares a custom NumPy-only Gradient Boosting Regressor against scikit-learn's implementation.\\n\\n\"\n",
        "\n",
        "    report_content += \"## Hyperparameters Explored\\n\"\n",
        "    report_content += f\"- Learning Rates: {', '.join(map(str, learning_rates))}\\n\"\n",
        "    report_content += f\"- Number of Estimators: {', '.join(map(str, n_estimators_list))}\\n\"\n",
        "    report_content += f\"- Max Depth: {', '.join(map(str, max_depth_list))}\\n\"\n",
        "    report_content += \"- Cross-validation: 5-fold\\n\\n\"\n",
        "\n",
        "    report_content += \"## Custom GBM Results\\n\"\n",
        "    report_content += \"| LR | Estimators | Depth | MAE | RMSE |\\n\"\n",
        "    report_content += \"|----|------------|-------|-----|------|\\n\"\n",
        "    for r in custom_results:\n",
        "        report_content += f\"| {r['lr']} | {r['n_est']} | {r['depth']} | {r['mae']:.2f} | {r['rmse']:.2f} |\\n\"\n",
        "    report_content += \"\\n\"\n",
        "\n",
        "    report_content += \"## Scikit-learn GBM Results\\n\"\n",
        "    report_content += \"| LR | Estimators | Depth | MAE | RMSE |\\n\"\n",
        "    report_content += \"|----|------------|-------|-----|------|\\n\"\n",
        "    for r in sklearn_results:\n",
        "        report_content += f\"| {r['lr']} | {r['n_est']} | {r['depth']} | {r['mae']:.2f} | {r['rmse']:.2f} |\\n\"\n",
        "    report_content += \"\\n\"\n",
        "\n",
        "    report_content += \"## Best Performing Models\\n\"\n",
        "    report_content += \"Based on Mean Absolute Error (MAE):\\n\\n\"\n",
        "    report_content += f\"- **Custom GBM Best:** LR: {best_custom['lr']}, Estimators: {best_custom['n_est']}, Depth: {best_custom['depth']} | MAE: {best_custom['mae']:.2f}, RMSE: {best_custom['rmse']:.2f}\\n\"\n",
        "    report_content += f\"- **Scikit-learn GBM Best:** LR: {best_sklearn['lr']}, Estimators: {best_sklearn['n_est']}, Depth: {best_sklearn['depth']} | MAE: {best_sklearn['mae']:.2f}, RMSE: {best_sklearn['rmse']:.2f}\\n\"\n",
        "\n",
        "    report_filename = \"gbm_comparison_report.md\"\n",
        "    with open(report_filename, \"w\") as f:\n",
        "        f.write(report_content)\n",
        "    print(f\"\\nReport saved to {report_filename}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kev0dUlSAwVU",
        "outputId": "4a8c4065-e6d8-42f4-d420-41636bf56904"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating and saving dataset...\n",
            "Dataset saved to data/regression_dataset.csv\n",
            "Loading and preprocessing dataset...\n",
            "Dataset loaded with X shape: (500, 10), y shape: (500,)\n",
            "Running hyperparameter sweep with 5-fold CV...\n",
            "\n",
            "--- Custom GBM Results ---\n",
            "LR: 0.1, Estimators: 50, Depth: 2 => MAE: 50.57, RMSE: 64.64\n",
            "LR: 0.1, Estimators: 50, Depth: 4 => MAE: 41.34, RMSE: 53.13\n",
            "LR: 0.1, Estimators: 100, Depth: 2 => MAE: 37.42, RMSE: 48.47\n",
            "LR: 0.1, Estimators: 100, Depth: 4 => MAE: 36.16, RMSE: 46.70\n",
            "LR: 0.1, Estimators: 200, Depth: 2 => MAE: 30.13, RMSE: 39.12\n",
            "LR: 0.1, Estimators: 200, Depth: 4 => MAE: 34.99, RMSE: 45.05\n",
            "LR: 0.05, Estimators: 50, Depth: 2 => MAE: 66.28, RMSE: 83.75\n",
            "LR: 0.05, Estimators: 50, Depth: 4 => MAE: 53.18, RMSE: 67.39\n",
            "LR: 0.05, Estimators: 100, Depth: 2 => MAE: 50.45, RMSE: 64.71\n",
            "LR: 0.05, Estimators: 100, Depth: 4 => MAE: 41.38, RMSE: 53.18\n",
            "LR: 0.05, Estimators: 200, Depth: 2 => MAE: 36.92, RMSE: 47.98\n",
            "LR: 0.05, Estimators: 200, Depth: 4 => MAE: 35.87, RMSE: 46.32\n",
            "LR: 0.01, Estimators: 50, Depth: 2 => MAE: 94.73, RMSE: 119.36\n",
            "LR: 0.01, Estimators: 50, Depth: 4 => MAE: 87.49, RMSE: 109.76\n",
            "LR: 0.01, Estimators: 100, Depth: 2 => MAE: 84.45, RMSE: 106.85\n",
            "LR: 0.01, Estimators: 100, Depth: 4 => MAE: 73.53, RMSE: 92.79\n",
            "LR: 0.01, Estimators: 200, Depth: 2 => MAE: 71.40, RMSE: 90.18\n",
            "LR: 0.01, Estimators: 200, Depth: 4 => MAE: 58.05, RMSE: 73.54\n",
            "\n",
            "--- Scikit-learn GBM Results ---\n",
            "LR: 0.1, Estimators: 50, Depth: 2 => MAE: 51.02, RMSE: 64.76\n",
            "LR: 0.1, Estimators: 50, Depth: 4 => MAE: 42.65, RMSE: 54.58\n",
            "LR: 0.1, Estimators: 100, Depth: 2 => MAE: 37.90, RMSE: 48.94\n",
            "LR: 0.1, Estimators: 100, Depth: 4 => MAE: 38.18, RMSE: 48.72\n",
            "LR: 0.1, Estimators: 200, Depth: 2 => MAE: 30.37, RMSE: 39.15\n",
            "LR: 0.1, Estimators: 200, Depth: 4 => MAE: 37.12, RMSE: 47.36\n",
            "LR: 0.05, Estimators: 50, Depth: 2 => MAE: 66.37, RMSE: 83.64\n",
            "LR: 0.05, Estimators: 50, Depth: 4 => MAE: 52.71, RMSE: 66.94\n",
            "LR: 0.05, Estimators: 100, Depth: 2 => MAE: 50.62, RMSE: 64.31\n",
            "LR: 0.05, Estimators: 100, Depth: 4 => MAE: 42.41, RMSE: 53.84\n",
            "LR: 0.05, Estimators: 200, Depth: 2 => MAE: 37.65, RMSE: 48.38\n",
            "LR: 0.05, Estimators: 200, Depth: 4 => MAE: 37.80, RMSE: 47.89\n",
            "LR: 0.01, Estimators: 50, Depth: 2 => MAE: 94.71, RMSE: 119.70\n",
            "LR: 0.01, Estimators: 50, Depth: 4 => MAE: 86.50, RMSE: 109.18\n",
            "LR: 0.01, Estimators: 100, Depth: 2 => MAE: 84.36, RMSE: 107.23\n",
            "LR: 0.01, Estimators: 100, Depth: 4 => MAE: 72.07, RMSE: 91.38\n",
            "LR: 0.01, Estimators: 200, Depth: 2 => MAE: 71.75, RMSE: 90.67\n",
            "LR: 0.01, Estimators: 200, Depth: 4 => MAE: 57.27, RMSE: 72.88\n",
            "\n",
            "--- Best Models ---\n",
            "Custom GBM Best: LR: 0.1, Estimators: 200, Depth: 2 | MAE: 30.13, RMSE: 39.12\n",
            "Scikit-learn GBM Best: LR: 0.1, Estimators: 200, Depth: 2 | MAE: 30.37, RMSE: 39.15\n",
            "\n",
            "Report saved to gbm_comparison_report.md\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}